{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbf530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr\n",
    "\n",
    "# https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda\n",
    "\n",
    "# -> https://github.com/markusbuchholz/deep-reinforcement-learning/blob/master/dqn/solution/dqn_agent.py\n",
    "# -> https://markus-x-buchholz.medium.com/deep-reinforcement-learning-introduction-deep-q-network-dqn-algorithm-fb74bf4d6862\n",
    "\n",
    "\n",
    "# https://github.com/deligentfool/dqn_zoo/blob/master/DDQN/ddqn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2a7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84fd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b9f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86eea928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  64\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "# If you got that error after a registration :\n",
    "# Error: Cannot re-register id: FrozenLakeNotSlippery-v0\n",
    "# So you need to delete an env registered\n",
    "\n",
    "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
    "\n",
    "for env in env_dict:\n",
    "    if 'FrozenLakeNotSlippery-v0' in env:\n",
    "        print(\"Remove {} from registry\".format(env))\n",
    "        del gym.envs.registration.registry.env_specs[env]\n",
    "\n",
    "\n",
    "register(\n",
    "   id=\"FrozenLakeNotSlippery-v0\",\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "print('Number of states: ', env.observation_space.n)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27eb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of Holes = [19, 29, 35, 41, 42, 46, 49, 52, 54, 59]\n"
     ]
    }
   ],
   "source": [
    "def get_index_of_holes(an_env):\n",
    "    \n",
    "    env_list = [[c.decode(\"utf-8\") for c in line] for line in an_env.desc]\n",
    "    index_holes = []\n",
    "    \n",
    "    size = np.int64(np.sqrt(an_env.observation_space.n))\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            c = env_list[i][j]\n",
    "            if (c == 'H'):\n",
    "                index_holes.append(i*size+j)\n",
    "    return index_holes\n",
    "\n",
    "\n",
    "l = get_index_of_holes(env)\n",
    "\n",
    "print(\"Indexes of Holes = {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a665c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed -size buffe to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b0a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.observation_space_size = state_size\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        obs_emb = F.one_hot(state.long(), num_classes=self.observation_space_size).squeeze()\n",
    "        x = F.relu(self.fc1(obs_emb.float()))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9f949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action, env, indexes_of_holes):\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    index_of_goal = env.observation_space.n - 1\n",
    "    # Reward function\n",
    "    # if new_state is a Hole\n",
    "    #if new_state in [5, 7, 11, 12]:\n",
    "    if new_state in indexes_of_holes:\n",
    "        reward = -1\n",
    "    # else if new_state is the Goal (Final State)\n",
    "    elif new_state == index_of_goal:\n",
    "        reward = 1\n",
    "    # else penalize research\n",
    "    else:\n",
    "        reward = -0.01\n",
    "    return new_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecd9a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(self.state_size, self.action_size)\n",
    "        self.qnetwork_target = QNetwork(self.state_size, self.action_size)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "        \n",
    "    def choose_action(self, s):\n",
    "        if (np.random.rand(1) < 0.1): \n",
    "            #print(\"sample action !\")\n",
    "            #return self.env.action_space.sample()\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            #agent_out = self.qnetwork_local(torch.tensor(s)).detach()\n",
    "            \n",
    "            self.qnetwork_local.eval() #puts network in evaluation mode\n",
    "            with torch.no_grad():\n",
    "                agent_out = self.qnetwork_local(torch.tensor(s))\n",
    "            self.qnetwork_local.train() #puts network back in training mode\n",
    "            \n",
    "            \n",
    "            #print(agent_out)\n",
    "            index_max = np.argmax(agent_out)\n",
    "            #print(\"index max = {}\".format(index_max))\n",
    "            #print(\"torch max action !\")\n",
    "            return index_max.item()    \n",
    "        \n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "agent = Agent(state_size=env.observation_space.n, action_size=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7890c95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -1.96\n",
      "Episode 200\tAverage Score: -1.94\n",
      "Episode 300\tAverage Score: -1.91\n",
      "Episode 400\tAverage Score: -1.89\n",
      "Episode 500\tAverage Score: -1.60\n",
      "Episode 600\tAverage Score: -0.25\n",
      "Episode 700\tAverage Score: 0.080\n",
      "Episode 800\tAverage Score: 0.17\n",
      "Episode 900\tAverage Score: 0.15\n",
      "Episode 1000\tAverage Score: 0.34\n",
      "Episode 1100\tAverage Score: 0.26\n",
      "Episode 1200\tAverage Score: 0.30\n",
      "Episode 1300\tAverage Score: 0.53\n",
      "Episode 1400\tAverage Score: 0.45\n",
      "Episode 1500\tAverage Score: 0.37\n",
      "Episode 1600\tAverage Score: 0.37\n",
      "Episode 1700\tAverage Score: 0.45\n",
      "Episode 1800\tAverage Score: 0.42\n",
      "Episode 1900\tAverage Score: 0.48\n",
      "Episode 2000\tAverage Score: 0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=200):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    indexes_of_holes = get_index_of_holes(env)\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    #eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = take_action(action, env, indexes_of_holes)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        #eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \"\"\"if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\"\"\"\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc4571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FF\u001b[41mF\u001b[0mFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFF\u001b[41mF\u001b[0mFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFF\u001b[41mF\u001b[0mHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFH\u001b[41mF\u001b[0mHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "i = 1\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FF\u001b[41mF\u001b[0mFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFF\u001b[41mF\u001b[0mFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFF\u001b[41mF\u001b[0mHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFH\u001b[41mF\u001b[0mHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "i = 2\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FF\u001b[41mF\u001b[0mFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFF\u001b[41mF\u001b[0mFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFF\u001b[41mF\u001b[0mHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFH\u001b[41mF\u001b[0mHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"i = {}\".format(i))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "            agent_out = agent.qnetwork_local(torch.tensor(state)).detach()\n",
    "            index_max = np.argmax(agent_out)\n",
    "            action =  index_max.item()    \n",
    "            env.render()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fae33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** SOME TESTS *****************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d81ae944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "action = agent.choose_action(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6de6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4ef65e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state of E is 2\n",
      "The action of E is 3\n",
      "The reward of E is -1\n",
      "The next state of E is 5\n",
      "The done of E is False\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/namedtuple-in-python/\n",
    "# Declaring namedtuple()  \n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "\n",
    "# Adding values  \n",
    "E = Experience(2,3,-1,5,False)  \n",
    "      \n",
    "# Access using index  \n",
    "print (\"The state of E is {}\".format(E.state))\n",
    "print (\"The action of E is {}\".format(E.action))\n",
    "print (\"The reward of E is {}\".format(E.reward))\n",
    "print (\"The next state of E is {}\".format(E.next_state))\n",
    "print (\"The done of E is {}\".format(E.done))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3c52e532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy vstack : Stack arrays in sequence vertically (row wise).\n",
    "# https://scipython.com/book/chapter-6-numpy/examples/vstack-and-hstack/\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "np.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "03405c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ACTION_SIZE = 4\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "buffer = ReplayBuffer(ACTION_SIZE, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8 , 9 , 10, 11, 12, 13, 14, 15]\n",
    "actions = [0, 1, 2, 3, 0, 2, 1, 3, 0 , 0 , 1, 1, 2, 3, 3, 1]\n",
    "rewards = [0, 1, 2, 3, 4, 5, 6, 7, 8 , 9 , 10, 11, 12, 13, 14, 15]\n",
    "next_states = [0, 1, 2, 3, 4, 5, 6, 7, 8 , 9 , 10, 11, 12, 13, 14, 15]\n",
    "dones = [False, False, False, False, False, False, False, False, False, False,False, False, False, False, False,False]\n",
    "for i in range(16):\n",
    "    buffer.add(states[i], actions[i], rewards[i], next_states[i], dones[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c6d3696e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.],\n",
      "        [14.],\n",
      "        [13.],\n",
      "        [ 8.],\n",
      "        [11.]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1]])\n",
      "tensor([[ 0.],\n",
      "        [14.],\n",
      "        [13.],\n",
      "        [ 8.],\n",
      "        [11.]])\n",
      "tensor([[ 0.],\n",
      "        [14.],\n",
      "        [13.],\n",
      "        [ 8.],\n",
      "        [11.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "s, a, r, ns, d = buffer.sample()\n",
    "print(s)\n",
    "print(a)\n",
    "print(r)\n",
    "print(ns)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa8dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cc8615db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "state = torch.tensor(2).long().unsqueeze(0)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8ddab36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "action = agent.choose_action(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2cd2612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.],\n",
      "        [14.],\n",
      "        [13.],\n",
      "        [ 8.],\n",
      "        [11.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ns)\n",
    "F.one_hot(ns.long(), num_classes=16).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "71a63a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.arange(4))\n",
    "\n",
    "random.choice(np.arange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "67d8cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.],\n",
      "        [ 9.],\n",
      "        [11.],\n",
      "        [15.],\n",
      "        [ 3.]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3]])\n",
      "tensor([[ 0.],\n",
      "        [ 9.],\n",
      "        [11.],\n",
      "        [15.],\n",
      "        [ 3.]])\n",
      "tensor([[ 0.],\n",
      "        [ 9.],\n",
      "        [11.],\n",
      "        [15.],\n",
      "        [ 3.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "s, a, r, ns, d = buffer.sample()\n",
    "print(s)\n",
    "print(a)\n",
    "print(r)\n",
    "print(ns)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9f1658a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ns = tensor([[ 0.],\n",
      "        [ 9.],\n",
      "        [11.],\n",
      "        [15.],\n",
      "        [ 3.]])\n",
      "values = tensor([[ 0.8663,  0.8960,  0.8958,  0.8663],\n",
      "        [ 0.9194,  0.9604,  0.9603, -0.9999],\n",
      "        [ 0.6083,  0.8229,  0.6270,  0.9469],\n",
      "        [ 0.5293,  0.6252,  0.2518,  0.4082],\n",
      "        [ 0.9195, -1.0003,  0.8959,  0.8959]])\n",
      "vals = tensor([0.8960, 0.9604, 0.9469, 0.6252, 0.9195]), row_idx = tensor([1, 1, 3, 1, 0])\n",
      "max_values_per_lines = tensor([0.8960, 0.9604, 0.9469, 0.6252, 0.9195])\n",
      "tensor([[0.8960],\n",
      "        [0.9604],\n",
      "        [0.9469],\n",
      "        [0.6252],\n",
      "        [0.9195]])\n"
     ]
    }
   ],
   "source": [
    "print(\"ns = {}\".format(ns))\n",
    "values = agent.qnetwork_target(ns).detach()\n",
    "print(\"values = {}\".format(values))\n",
    "vals, row_idx = values.max(1)\n",
    "print(\"vals = {}, row_idx = {}\".format(vals, row_idx))\n",
    "\n",
    "max_values_per_lines = values.max(1)[0]\n",
    "print(\"max_values_per_lines = {}\".format(max_values_per_lines))\n",
    "\n",
    "print(max_values_per_lines.unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "efaeacd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_values = tensor([[ 0.8772,  0.8981,  0.9028,  0.8818],\n",
      "        [ 0.9197,  0.9622,  0.9671, -1.0005],\n",
      "        [ 0.6118,  0.8234,  0.6293,  0.9541],\n",
      "        [ 0.5296,  0.6284,  0.2552,  0.4142],\n",
      "        [ 0.9193, -1.0011,  0.9063,  0.9066]], grad_fn=<AddmmBackward>)\n",
      "actions = tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8772],\n",
       "        [0.9197],\n",
       "        [0.8234],\n",
       "        [0.6284],\n",
       "        [0.9066]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_values =  agent.qnetwork_local(ns)\n",
    "print(\"local_values = {}\".format(local_values))\n",
    "print(\"actions = {}\".format(a))\n",
    "local_values.gather(1,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f936be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06460524])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1afa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb372410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(np.arange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0078cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
