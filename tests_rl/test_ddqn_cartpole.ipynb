{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr\n",
    "\n",
    "# based on : https://github.com/markusbuchholz/deep-reinforcement-learning/blob/master/dqn/solution/dqn_agent.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3464d87f",
   "metadata": {},
   "source": [
    "CartPole\n",
    "\n",
    "Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum\n",
    "        starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num\tObservation               Min             Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                -24 deg         24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is\n",
    "        pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the\n",
    "        cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f96934e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6358c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed -size buffe to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91140cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.observation_space_size = state_size\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24b71e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(self.state_size, self.action_size)\n",
    "        self.qnetwork_target = QNetwork(self.state_size, self.action_size)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "        \n",
    "    def choose_action(self, s):\n",
    "        s = torch.from_numpy(s).float().unsqueeze(0)\n",
    "        \n",
    "        if (np.random.rand(1) < 0.1): \n",
    "            #print(\"sample action !\")\n",
    "            #return self.env.action_space.sample()\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            agent_out = self.qnetwork_local(s).detach()\n",
    "            #print(agent_out)\n",
    "            index_max = np.argmax(agent_out)\n",
    "            #print(\"index max = {}\".format(index_max))\n",
    "            #print(\"torch max action !\")\n",
    "            return index_max.item()    \n",
    "        \n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "   \n",
    "\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf94a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 9.86\n",
      "Episode 200\tAverage Score: 10.35\n",
      "Episode 300\tAverage Score: 9.961\n",
      "Episode 400\tAverage Score: 9.97\n",
      "Episode 500\tAverage Score: 9.740\n",
      "Episode 600\tAverage Score: 9.85\n",
      "Episode 700\tAverage Score: 9.940\n",
      "Episode 800\tAverage Score: 10.03\n",
      "Episode 900\tAverage Score: 10.77\n",
      "Episode 1000\tAverage Score: 11.24\n",
      "Episode 1100\tAverage Score: 13.60\n",
      "Episode 1200\tAverage Score: 17.65\n",
      "Episode 1300\tAverage Score: 27.92\n",
      "Episode 1400\tAverage Score: 83.37\n",
      "Episode 1462\tAverage Score: 200.92\n",
      "Environment solved in 1362 episodes!\tAverage Score: 200.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    #eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        #eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "148f806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 1\n",
      "i = 2\n"
     ]
    }
   ],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"i = {}\".format(i))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            agent_out = agent.qnetwork_local(state).detach()\n",
    "            index_max = np.argmax(agent_out)\n",
    "            action =  index_max.item()    \n",
    "            env.render()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf5532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
