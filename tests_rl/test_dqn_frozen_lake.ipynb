{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laurent LEQUIEVRE\n",
    "# Research Engineer, CNRS (France)\n",
    "# Institut Pascal UMR6602\n",
    "# laurent.lequievre@uca.fr\n",
    "\n",
    "# Solution based on :\n",
    "# https://www.kaggle.com/wuhao1542/pytorch-rl-0-frozenlake-q-network-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e486735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e43df71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5344909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove FrozenLakeNotSlippery-v0 from registry\n"
     ]
    }
   ],
   "source": [
    "# If you got that error after a registration :\n",
    "# Error: Cannot re-register id: FrozenLakeNotSlippery-v0\n",
    "# So you need to delete an env registered\n",
    "\n",
    "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
    "\n",
    "for env in env_dict:\n",
    "    if 'FrozenLakeNotSlippery-v0' in env:\n",
    "        print(\"Remove {} from registry\".format(env))\n",
    "        del gym.envs.registration.registry.env_specs[env]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "267aee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "   id=\"FrozenLakeNotSlippery-v0\",\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    ")\n",
    "\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2296e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space = 16, action space = 4\n"
     ]
    }
   ],
   "source": [
    "observation_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "print(\"observation space = {}, action space = {}\".format(observation_space, action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11fe159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131c6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_linear_layer(linear_layer):\n",
    "    linear_layer.weight.data.uniform_()\n",
    "    linear_layer.bias.data.fill_(-0.02)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, observation_space_size, action_space_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.observation_space_size = observation_space_size\n",
    "        self.hidden_size = observation_space_size\n",
    "        self.l1 = nn.Linear(in_features=observation_space_size, out_features=self.hidden_size)\n",
    "        self.l2 = nn.Linear(in_features=self.hidden_size, out_features=action_space_size)\n",
    "        uniform_linear_layer(self.l1)\n",
    "        uniform_linear_layer(self.l2)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        obs_emb = F.one_hot(torch.LongTensor([int(state)]), num_classes=self.observation_space_size)\n",
    "        out1 = torch.sigmoid(self.l1(obs_emb.float()))\n",
    "        return self.l2(out1).view((-1)) # 1 x ACTION_SPACE_SIZE == 1 x 4  =>  4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35fa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action, env):\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    # Reward function\n",
    "    # if new_state is a Hole\n",
    "    if new_state in [5, 7, 11, 12]:\n",
    "        reward = -1\n",
    "    # else if new_state is the Goal (Final State)\n",
    "    elif new_state == 15:\n",
    "        reward = 1\n",
    "    # else penalize research\n",
    "    else:\n",
    "        reward = -0.01\n",
    "    return new_state, reward, done, info\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, env):\n",
    "        self.agent = Agent(env.observation_space.n, env.action_space.n)\n",
    "        self.optimizer = optim.Adam(params=self.agent.parameters())\n",
    "        self.env = env\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        for i in range(epoch):\n",
    "            print('.', end='')\n",
    "            current_state = self.env.reset()\n",
    "            j = 0\n",
    "            while j < 200:\n",
    "                # perform chosen action\n",
    "                an_action = self.choose_action(current_state)\n",
    "                next_state, reward, done, _ = take_action(an_action,self.env)\n",
    "                \n",
    "                # calculate target and loss\n",
    "                target_q = reward + self.gamma * torch.max(self.agent(next_state).detach()) # detach from the computing flow\n",
    "                loss = F.smooth_l1_loss(self.agent(current_state)[an_action], target_q)\n",
    "                \n",
    "                # update model to optimize Q\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # update state\n",
    "                current_state = next_state\n",
    "                j += 1\n",
    "                if done == True: break\n",
    "            \n",
    "           \n",
    "        print(\"Train is done !\")\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        if (np.random.rand(1) < 0.1): \n",
    "            #print(\"sample action !\")\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            agent_out = self.agent(s).detach()\n",
    "            #print(agent_out)\n",
    "            index_max = np.argmax(agent_out)\n",
    "            #print(\"index max = {}\".format(index_max))\n",
    "            #print(\"torch max action !\")\n",
    "            return index_max.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a521c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Train is done !\n"
     ]
    }
   ],
   "source": [
    "# Use Trainer class to train Agent network\n",
    "t = Trainer(env)\n",
    "t.train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a568e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test the best solution from an Agent (with a neural network)\n",
    "# Initial state = 0, Final state = 15\n",
    "current_state = env.reset()  # S is the initial state = 0\n",
    "env.render()\n",
    "\n",
    "while (current_state != 15):\n",
    "  agent_out = t.agent(current_state).detach()   \n",
    "  an_action = np.argmax(agent_out).item()\n",
    "  print(\"Take action {}\".format(an_action))\n",
    "  current_state, _, _, _ = env.step(an_action)\n",
    "  env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea91e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove FrozenLakeNotSlippery-v1 from registry\n"
     ]
    }
   ],
   "source": [
    "# If you got that error after a registration :\n",
    "# Error: Cannot re-register id: FrozenLakeNotSlippery-v1\n",
    "# So you need to delete an env registered\n",
    "\n",
    "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
    "\n",
    "for env in env_dict:\n",
    "    if 'FrozenLakeNotSlippery-v1' in env:\n",
    "        print(\"Remove {} from registry\".format(env))\n",
    "        del gym.envs.registration.registry.env_specs[env]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "239dbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "   id=\"FrozenLakeNotSlippery-v1\",\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    ")\n",
    "\n",
    "env2 = gym.make(\"FrozenLakeNotSlippery-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50af5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env2.reset()\n",
    "env2.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3350a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new take_action function for a frozen lake 8x8\n",
    "def take_action(action, env):\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    # Reward function\n",
    "    # if new_state is a Hole\n",
    "    if new_state in [19, 29, 35, 41, 42, 46, 49, 52, 54, 59]:\n",
    "        reward = -1\n",
    "     # else if new_state is the Goal (Final State)\n",
    "    elif new_state == 63:\n",
    "        reward = 1\n",
    "    # else penalize research\n",
    "    else:\n",
    "        reward = -0.01\n",
    "    return new_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d69969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Train is done !\n"
     ]
    }
   ],
   "source": [
    "# Use Trainer class to train Agent network\n",
    "t = Trainer(env2)\n",
    "t.train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a16bd33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mF\u001b[0mFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FF\u001b[41mF\u001b[0mFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFF\u001b[41mF\u001b[0mFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHF\u001b[41mF\u001b[0mFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFF\u001b[41mF\u001b[0mHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFH\u001b[41mF\u001b[0mHF\n",
      "FFFHFFFG\n",
      "Take action 1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHF\u001b[41mF\u001b[0mFG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFF\u001b[41mF\u001b[0mG\n",
      "Take action 2\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test the best solution from an Agent (with a neural network)\n",
    "# Initial state = 0, Final state = 63\n",
    "current_state = env2.reset()  # S is the initial state = 0\n",
    "env2.render()\n",
    "\n",
    "while (current_state != 63):\n",
    "  agent_out = t.agent(current_state).detach()   \n",
    "  an_action = np.argmax(agent_out).item()\n",
    "  print(\"Take action {}\".format(an_action))\n",
    "  current_state, _, _, _ = env2.step(an_action)\n",
    "  env2.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17497066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
